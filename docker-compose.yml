# ============================================
# Roman Numeral Service - Docker Compose
# ============================================
# Full observability + data engineering stack
# Just run: docker-compose up -d
#
# Components:
# - Application (Spring Boot + Java 21)
# - PostgreSQL (Database - OLTP)
# - Kafka (Event streaming - Bronze layer ingestion)
# - MinIO (S3-compatible object storage)
# - Hive Metastore (Iceberg catalog)
# - Flink (Streaming engine - Bronze layer)
# - Spark (Batch engine - Silver/Gold layers)
# - Airflow (Orchestration - ETL scheduling)
# - Prometheus (Metrics collection)
# - Loki + Promtail (Log aggregation)
# - Grafana (Visualization)
# - Marquez (Data lineage with OpenLineage)
# - Superset (BI dashboards)
# - Jupyter (Data analysis notebooks)
#
# Note: Uses json-file logging driver (default) instead of
# Loki plugin. Promtail scrapes logs from Docker log files.
# ============================================

services:
  # ==========================================
  # PostgreSQL Database
  # ==========================================
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: romannumeral
      POSTGRES_USER: romannumeral
      POSTGRES_PASSWORD: romannumeral_secret
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U romannumeral -d romannumeral"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  # ==========================================
  # Apache Kafka (Event Streaming)
  # ==========================================
  # Using KRaft mode (no Zookeeper required)
  # Publishes conversion events for data lake ingestion
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,EXTERNAL://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - observability

  # ==========================================
  # MinIO (S3-Compatible Object Storage)
  # ==========================================
  # Used as the storage layer for the data lakehouse
  # Stores Iceberg table data (Parquet files)
  minio:
    image: minio/minio:RELEASE.2024-01-01T16-36-33Z
    container_name: minio
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - observability

  # Create default buckets for the lakehouse
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin123;
      mc mb myminio/lakehouse --ignore-existing;
      mc mb myminio/lakehouse-bronze --ignore-existing;
      mc mb myminio/lakehouse-silver --ignore-existing;
      mc mb myminio/lakehouse-gold --ignore-existing;
      mc anonymous set download myminio/lakehouse;
      exit 0;
      "
    networks:
      - observability

  # ==========================================
  # Hive Metastore Database (PostgreSQL)
  # ==========================================
  # Separate PostgreSQL instance for Hive Metastore
  # Stores Iceberg table metadata
  hive-metastore-db:
    image: postgres:16-alpine
    container_name: hive-metastore-db
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive_secret
    volumes:
      - hive_metastore_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  # ==========================================
  # Hive Metastore (Iceberg Catalog)
  # ==========================================
  # Provides catalog service for Iceberg tables
  # Used by Spark/Flink for table discovery and schema management
  # Using 3.1.3 for better Iceberg compatibility (well-documented)
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    ports:
      - "9083:9083"   # Thrift port for Spark/Trino connections
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-db:5432/metastore
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive_secret
    volumes:
      - ./docker/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - hive_warehouse:/opt/hive/data/warehouse
    depends_on:
      hive-metastore-db:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "cat < /dev/null > /dev/tcp/localhost/9083"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - observability

  # ==========================================
  # Apache Flink JobManager
  # ==========================================
  # Streaming engine for Bronze layer ingestion
  # UI available at http://localhost:8092
  # Handles: exactly-once, watermarks, late arrivals, DLQ
  flink-jobmanager:
    image: flink:1.18-java17
    container_name: flink-jobmanager
    ports:
      - "8092:8081"   # Flink Web UI
      - "6123:6123"   # RPC port
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        state.backend: rocksdb
        state.backend.rocksdb.localdir: /tmp/flink-rocksdb
        state.checkpoints.dir: s3://lakehouse/flink/checkpoints
        state.savepoints.dir: s3://lakehouse/flink/savepoints
        execution.checkpointing.interval: 60000
        execution.checkpointing.mode: EXACTLY_ONCE
        execution.checkpointing.min-pause: 30000
        execution.checkpointing.timeout: 600000
        restart-strategy: fixed-delay
        restart-strategy.fixed-delay.attempts: 3
        restart-strategy.fixed-delay.delay: 30s
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        s3.access-key: minioadmin
        s3.secret-key: minioadmin123
    volumes:
      - ./data-platform/flink/bronze-ingestion:/opt/flink-jobs/bronze-ingestion:ro
      - ./data-platform/flink/conf/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml:ro
      - flink_data:/tmp/flink-rocksdb
    command: jobmanager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - observability

  # ==========================================
  # Apache Flink TaskManager
  # ==========================================
  # Executes Flink streaming tasks
  flink-taskmanager:
    image: flink:1.18-java17
    container_name: flink-taskmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        taskmanager.memory.process.size: 2g
        state.backend: rocksdb
        state.backend.rocksdb.localdir: /tmp/flink-rocksdb
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        s3.access-key: minioadmin
        s3.secret-key: minioadmin123
    volumes:
      - ./data-platform/flink/bronze-ingestion:/opt/flink-jobs/bronze-ingestion:ro
      - flink_data:/tmp/flink-rocksdb
    command: taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    networks:
      - observability

  # ==========================================
  # Apache Spark Master
  # ==========================================
  # Batch processing engine for Silver/Gold layers
  # UI available at http://localhost:8090
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    ports:
      - "8090:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master port
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    volumes:
      - ./data-platform/spark/jobs:/opt/spark-jobs:ro
      - ./data-platform/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - spark_data:/opt/spark/work
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - observability

  # ==========================================
  # Apache Spark Worker
  # ==========================================
  # Executes Spark batch tasks for Silver/Gold ETL
  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    ports:
      - "8091:8081"   # Spark Worker UI
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
    volumes:
      - ./data-platform/spark/jobs:/opt/spark-jobs:ro
      - ./data-platform/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - spark_data:/opt/spark/work
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - observability

  # ==========================================
  # Apache Airflow (Orchestration)
  # ==========================================
  # Schedules and monitors Silver/Gold ETL jobs
  # UI available at http://localhost:8093
  airflow:
    image: apache/airflow:2.8.1-python3.11
    container_name: airflow
    ports:
      - "8093:8080"   # Airflow Web UI
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow_secret@airflow-db:5432/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-change-in-prod
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      # Spark connection
      - SPARK_MASTER_URL=spark://spark-master:7077
      # MinIO connection
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin123
      - AWS_ENDPOINT_URL=http://minio:9000
    volumes:
      - ./data-platform/airflow/dags:/opt/airflow/dags:ro
      - ./data-platform/airflow/plugins:/opt/airflow/plugins:ro
      - ./data-platform/airflow/logs:/opt/airflow/logs
      - ./data-platform/spark/jobs:/opt/spark-jobs:ro
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        airflow webserver & airflow scheduler
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      airflow-db:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    networks:
      - observability

  # Airflow Database (PostgreSQL)
  airflow-db:
    image: postgres:16-alpine
    container_name: airflow-db
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow_secret
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  # ==========================================
  # Roman Numeral Service (Main Application)
  # ==========================================
  roman-numeral-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: roman-numeral-service
    ports:
      - "8080:8080"   # Application
      - "8081:8081"   # Actuator/Management
    environment:
      # Use dev profile for local development/testing
      # Reviewers run docker-compose up and get the full experience
      - SPRING_PROFILES_ACTIVE=dev
      - JAVA_OPTS=-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0
      # Database connection (PostgreSQL in Docker)
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/romannumeral
      - SPRING_DATASOURCE_USERNAME=romannumeral
      - SPRING_DATASOURCE_PASSWORD=romannumeral_secret
      - SPRING_DATASOURCE_DRIVER=org.postgresql.Driver
      - SPRING_JPA_DATABASE_PLATFORM=org.hibernate.dialect.PostgreSQLDialect
      # Kafka connection
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8081/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - observability
    labels:
      - "app=roman-numeral-service"
      - "logging=promtail"
    # Use json-file logging driver (default)
    # Promtail scrapes logs from Docker's json-file logs
    # No plugin installation required - works out of the box
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "app,logging"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      loki:
        condition: service_started

  # ==========================================
  # Prometheus (Metrics Collection)
  # ==========================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    networks:
      - observability
    depends_on:
      - roman-numeral-service

  # ==========================================
  # Loki (Log Aggregation)
  # ==========================================
  loki:
    image: grafana/loki:2.9.2
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./docker/loki/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - observability

  # ==========================================
  # Promtail (Log Shipping to Loki)
  # ==========================================
  # Scrapes logs from Docker's json-file logging driver
  # No Docker plugin required - works out of the box
  promtail:
    image: grafana/promtail:2.9.2
    container_name: promtail
    volumes:
      - ./docker/loki/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - observability
    depends_on:
      - loki

  # ==========================================
  # Grafana (Visualization)
  # ==========================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    networks:
      - observability
    depends_on:
      - prometheus
      - loki

  # ==========================================
  # Marquez (Data Lineage with OpenLineage)
  # ==========================================
  # API: http://localhost:5000
  # Web UI: http://localhost:3001
  # OpenLineage endpoint: http://marquez:5000/api/v1/lineage
  # ==========================================
  
  marquez:
    image: marquezproject/marquez:0.47.0
    container_name: marquez
    ports:
      - "5050:5000"   # API (5050 to avoid macOS AirPlay conflict)
      - "5001:5001"   # Admin
    environment:
      MARQUEZ_CONFIG: /usr/src/app/marquez.yml
    volumes:
      - ./docker/marquez/marquez.yml:/usr/src/app/marquez.yml:ro
    depends_on:
      marquez-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/v1/namespaces"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - observability
    labels:
      logging: "promtail"

  marquez-db:
    image: postgres:16-alpine
    container_name: marquez-db
    environment:
      POSTGRES_DB: marquez
      POSTGRES_USER: marquez
      POSTGRES_PASSWORD: marquez_secret
    volumes:
      - marquez_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U marquez -d marquez"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  marquez-web:
    image: marquezproject/marquez-web:0.47.0
    container_name: marquez-web
    ports:
      - "3001:3000"
    environment:
      MARQUEZ_HOST: marquez
      MARQUEZ_PORT: 5000
    depends_on:
      marquez:
        condition: service_healthy
    networks:
      - observability
    labels:
      logging: "promtail"

  # ==========================================
  # Apache Superset (BI Dashboards)
  # ==========================================
  # Web UI: http://localhost:8088
  # Default login: admin / admin
  # ==========================================
  
  superset:
    image: apache/superset:3.1.0
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      # Database connection for Superset metadata
      SUPERSET_SECRET_KEY: "rns_superset_secret_key_change_in_prod"
      DATABASE_URL: "postgresql+psycopg2://superset:superset_secret@superset-db:5432/superset"
      REDIS_URL: "redis://superset-redis:6379/0"
      # Flask settings
      FLASK_ENV: development
      SUPERSET_LOAD_EXAMPLES: "no"
    volumes:
      - ./docker/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - superset_home:/app/superset_home
    command: >
      bash -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload
      "
    depends_on:
      superset-db:
        condition: service_healthy
      superset-redis:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - observability
    labels:
      logging: "promtail"

  superset-db:
    image: postgres:16-alpine
    container_name: superset-db
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset_secret
    volumes:
      - superset_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset -d superset"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  superset-redis:
    image: redis:7-alpine
    container_name: superset-redis
    volumes:
      - superset_redis_data:/data
    networks:
      - observability

  # ==========================================
  # Jupyter Notebook (Data Analysis)
  # ==========================================
  # Web UI: http://localhost:8888
  # Token: Check logs or use 'jupyter' as password
  # ==========================================
  
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.5.0
    container_name: jupyter
    ports:
      - "8888:8888"
    environment:
      # Jupyter settings
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: "jupyter"
      # Spark settings for connecting to cluster
      SPARK_MASTER: "spark://spark-master:7077"
      # MinIO/S3 settings
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin123"
      AWS_ENDPOINT_URL: "http://minio:9000"
    volumes:
      - ./data-platform/notebooks:/home/jovyan/work
      - jupyter_data:/home/jovyan/.local
    user: root
    command: >
      bash -c "
        pip install --quiet psycopg2-binary sqlalchemy pandas-profiling ydata-profiling great-expectations pyiceberg &&
        start-notebook.sh --NotebookApp.token='jupyter'
      "
    networks:
      - observability
    labels:
      logging: "promtail"

# ==========================================
# Networks
# ==========================================
networks:
  observability:
    driver: bridge

# ==========================================
# Volumes
# ==========================================
volumes:
  postgres_data:
  kafka_data:
  minio_data:
  hive_metastore_db_data:
  hive_warehouse:
  flink_data:
  spark_data:
  airflow_db_data:
  prometheus_data:
  loki_data:
  grafana_data:
  marquez_db_data:
  superset_home:
  superset_db_data:
  superset_redis_data:
  jupyter_data:


# ============================================
# Roman Numeral Service - Docker Compose
# ============================================
# Full observability + data engineering stack
# Just run: docker-compose up -d
#
# Components:
# - Application (Spring Boot + Java 21)
# - PostgreSQL (Database - OLTP)
# - Kafka (Event streaming - Bronze layer ingestion)
# - MinIO (S3-compatible object storage)
# - Hive Metastore (Iceberg catalog)
# - Prometheus (Metrics collection)
# - Loki + Promtail (Log aggregation)
# - Grafana (Visualization)
#
# Note: Uses json-file logging driver (default) instead of
# Loki plugin. Promtail scrapes logs from Docker log files.
# ============================================

services:
  # ==========================================
  # PostgreSQL Database
  # ==========================================
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: romannumeral
      POSTGRES_USER: romannumeral
      POSTGRES_PASSWORD: romannumeral_secret
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U romannumeral -d romannumeral"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  # ==========================================
  # Apache Kafka (Event Streaming)
  # ==========================================
  # Using KRaft mode (no Zookeeper required)
  # Publishes conversion events for data lake ingestion
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      # KRaft settings (no Zookeeper)
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      # Listener settings
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Topic settings
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_NUM_PARTITIONS=3
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1
      # Performance
      - KAFKA_CFG_LOG_RETENTION_HOURS=168
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824
    volumes:
      - kafka_data:/bitnami/kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - observability

  # ==========================================
  # MinIO (S3-Compatible Object Storage)
  # ==========================================
  # Used as the storage layer for the data lakehouse
  # Stores Iceberg table data (Parquet files)
  minio:
    image: minio/minio:RELEASE.2024-01-01T16-36-33Z
    container_name: minio
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - observability

  # Create default buckets for the lakehouse
  minio-init:
    image: minio/mc:RELEASE.2024-01-01T16-36-33Z
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin123;
      mc mb myminio/lakehouse --ignore-existing;
      mc mb myminio/lakehouse-bronze --ignore-existing;
      mc mb myminio/lakehouse-silver --ignore-existing;
      mc mb myminio/lakehouse-gold --ignore-existing;
      mc anonymous set download myminio/lakehouse;
      exit 0;
      "
    networks:
      - observability

  # ==========================================
  # Hive Metastore Database (PostgreSQL)
  # ==========================================
  # Separate PostgreSQL instance for Hive Metastore
  # Stores Iceberg table metadata
  hive-metastore-db:
    image: postgres:16-alpine
    container_name: hive-metastore-db
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive_secret
    volumes:
      - hive_metastore_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - observability

  # ==========================================
  # Hive Metastore (Iceberg Catalog)
  # ==========================================
  # Provides catalog service for Iceberg tables
  # Used by Spark for table discovery and schema management
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    ports:
      - "9083:9083"   # Thrift port for Spark/Trino connections
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-db:5432/metastore
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive_secret
    volumes:
      - ./docker/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - hive_warehouse:/opt/hive/data/warehouse
    depends_on:
      hive-metastore-db:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "cat < /dev/null > /dev/tcp/localhost/9083"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - observability

  # ==========================================
  # Roman Numeral Service (Main Application)
  # ==========================================
  roman-numeral-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: roman-numeral-service
    ports:
      - "8080:8080"   # Application
      - "8081:8081"   # Actuator/Management
    environment:
      # Use dev profile for local development/testing
      # Reviewers run docker-compose up and get the full experience
      - SPRING_PROFILES_ACTIVE=dev
      - JAVA_OPTS=-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0
      # Database connection (PostgreSQL in Docker)
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/romannumeral
      - SPRING_DATASOURCE_USERNAME=romannumeral
      - SPRING_DATASOURCE_PASSWORD=romannumeral_secret
      - SPRING_DATASOURCE_DRIVER=org.postgresql.Driver
      - SPRING_JPA_DATABASE_PLATFORM=org.hibernate.dialect.PostgreSQLDialect
      # Kafka connection
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8081/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - observability
    labels:
      - "app=roman-numeral-service"
      - "logging=promtail"
    # Use json-file logging driver (default)
    # Promtail scrapes logs from Docker's json-file logs
    # No plugin installation required - works out of the box
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "app,logging"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      loki:
        condition: service_started

  # ==========================================
  # Prometheus (Metrics Collection)
  # ==========================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    networks:
      - observability
    depends_on:
      - roman-numeral-service

  # ==========================================
  # Loki (Log Aggregation)
  # ==========================================
  loki:
    image: grafana/loki:2.9.2
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./docker/loki/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - observability

  # ==========================================
  # Promtail (Log Shipping to Loki)
  # ==========================================
  # Scrapes logs from Docker's json-file logging driver
  # No Docker plugin required - works out of the box
  promtail:
    image: grafana/promtail:2.9.2
    container_name: promtail
    volumes:
      - ./docker/loki/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - observability
    depends_on:
      - loki

  # ==========================================
  # Grafana (Visualization)
  # ==========================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    networks:
      - observability
    depends_on:
      - prometheus
      - loki

# ==========================================
# Networks
# ==========================================
networks:
  observability:
    driver: bridge

# ==========================================
# Volumes
# ==========================================
volumes:
  postgres_data:
  kafka_data:
  minio_data:
  hive_metastore_db_data:
  hive_warehouse:
  prometheus_data:
  loki_data:
  grafana_data:


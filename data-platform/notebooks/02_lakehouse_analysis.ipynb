{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèîÔ∏è Lakehouse Analysis with PySpark + Iceberg\n",
    "\n",
    "This notebook demonstrates querying the Iceberg Lakehouse tables using PySpark.\n",
    "\n",
    "## Medallion Architecture Layers\n",
    "- **Bronze**: Raw events from Kafka (ingested by Flink)\n",
    "- **Silver**: Cleaned, deduplicated facts and SCD Type 2 dimensions\n",
    "- **Gold**: Aggregated metrics for BI/Analytics\n",
    "\n",
    "## Prerequisites\n",
    "Run the medallion flow test first:\n",
    "```bash\n",
    "docker exec spark-master /opt/spark/bin/spark-submit \\\n",
    "    --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \\\n",
    "    --conf spark.extraListeners= \\\n",
    "    /opt/spark-jobs/test_medallion_flow.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Spark for Iceberg using REST Catalog\n",
    "# This connects to the same catalog used by Airflow DAGs\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lakehouse Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\"\n",
    "            \"org.apache.iceberg:iceberg-aws-bundle:1.5.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.warehouse\", \"s3://lakehouse/warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.s3.access-key-id\", \"minioadmin\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.s3.secret-access-key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.lakehouse.client.region\", \"us-east-1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"üì¶ Iceberg catalog: lakehouse (REST @ http://iceberg-rest:8181)\")\n",
    "print(f\"ü™£ MinIO endpoint: http://minio:9000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in each Medallion layer\n",
    "for layer in ['bronze', 'silver', 'gold']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÅ {layer.upper()} LAYER\")\n",
    "    print('='*60)\n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN lakehouse.{layer}\")\n",
    "        if tables.count() == 0:\n",
    "            print(\"   (no tables)\")\n",
    "        else:\n",
    "            for row in tables.collect():\n",
    "                table_name = f\"lakehouse.{layer}.{row['tableName']}\"\n",
    "                count = spark.table(table_name).count()\n",
    "                print(f\"   ‚îî‚îÄ‚îÄ {row['tableName']}: {count} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Gold layer tables\n",
    "print(\"üìä GOLD LAYER ANALYTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Daily Summary\n",
    "try:\n",
    "    daily_df = spark.table(\"lakehouse.gold.daily_conversion_summary\")\n",
    "    print(\"\\nüìÖ Daily Conversion Summary:\")\n",
    "    daily_df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå daily_conversion_summary: {e}\")\n",
    "\n",
    "# User Metrics\n",
    "try:\n",
    "    users_df = spark.table(\"lakehouse.gold.user_metrics\")\n",
    "    print(\"\\nüë• User Metrics:\")\n",
    "    users_df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå user_metrics: {e}\")\n",
    "\n",
    "# Popular Numbers\n",
    "try:\n",
    "    popular_df = spark.table(\"lakehouse.gold.popular_numbers\")\n",
    "    print(\"\\nüî• Most Popular Numbers:\")\n",
    "    popular_df.show(truncate=False)\n",
    "    \n",
    "    # Visualization\n",
    "    pdf = popular_df.toPandas()\n",
    "    if len(pdf) > 0:\n",
    "        # Take top 20 for cleaner visualization\n",
    "        pdf_top = pdf.head(20)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.barh(pdf_top['output_roman'].astype(str) + ' (' + pdf_top['input_number'].astype(str) + ')', \n",
    "                pdf_top['request_count'], color='steelblue')\n",
    "        ax.set_xlabel('Request Count')\n",
    "        ax.set_ylabel('Roman Numeral (Number)')\n",
    "        ax.set_title('Top 20 Most Popular Roman Numeral Conversions')\n",
    "        ax.invert_yaxis()  # Highest at top\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå popular_numbers: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç INTERESTING GOLD LAYER ANALYTICS\n",
    "print(\"=\"*60)\n",
    "print(\"üéØ ADVANCED ANALYTICS ON GOLD LAYER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. What percentage of conversions are for numbers under 100?\n",
    "print(\"\\nüìä 1. Distribution by Number Range:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN input_number <= 10 THEN '1-10 (Small)'\n",
    "            WHEN input_number <= 100 THEN '11-100 (Medium)'\n",
    "            WHEN input_number <= 1000 THEN '101-1000 (Large)'\n",
    "            ELSE '1001-3999 (Very Large)'\n",
    "        END as number_range,\n",
    "        SUM(request_count) as total_requests,\n",
    "        ROUND(SUM(request_count) * 100.0 / (SELECT SUM(request_count) FROM lakehouse.gold.popular_numbers), 2) as percentage\n",
    "    FROM lakehouse.gold.popular_numbers\n",
    "    GROUP BY 1\n",
    "    ORDER BY MIN(input_number)\n",
    "\"\"\").show()\n",
    "\n",
    "# 2. Which Roman numerals have the most characters?\n",
    "print(\"\\nüìè 2. Longest Roman Numerals Requested:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        input_number,\n",
    "        output_roman,\n",
    "        LENGTH(output_roman) as numeral_length,\n",
    "        request_count\n",
    "    FROM lakehouse.gold.popular_numbers\n",
    "    ORDER BY LENGTH(output_roman) DESC, request_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 3. \"Round number\" preference (multiples of 10, 100, etc.)\n",
    "print(\"\\nüéØ 3. Round Number Preference:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN input_number % 100 = 0 THEN 'Multiple of 100'\n",
    "            WHEN input_number % 50 = 0 THEN 'Multiple of 50'\n",
    "            WHEN input_number % 10 = 0 THEN 'Multiple of 10'\n",
    "            WHEN input_number % 5 = 0 THEN 'Multiple of 5'\n",
    "            ELSE 'Other'\n",
    "        END as number_type,\n",
    "        COUNT(*) as unique_numbers,\n",
    "        SUM(request_count) as total_requests\n",
    "    FROM lakehouse.gold.popular_numbers\n",
    "    GROUP BY 1\n",
    "    ORDER BY total_requests DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 4. Numbers with repeated characters in Roman numerals (like III, XXX, CCC)\n",
    "print(\"\\nüîÅ 4. Numbers with Repeated Roman Numerals:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT input_number, output_roman, request_count\n",
    "    FROM lakehouse.gold.popular_numbers\n",
    "    WHERE output_roman RLIKE '^(I{1,3}|V|X{1,3}|L|C{1,3}|D|M{1,3})$'\n",
    "       OR output_roman IN ('III', 'XXX', 'CCC', 'MMM')\n",
    "    ORDER BY request_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# 5. Average response time trends (if available in user_metrics)\n",
    "print(\"\\n‚ö° 5. User Activity Summary:\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_users,\n",
    "            SUM(total_requests) as total_requests,\n",
    "            ROUND(AVG(total_requests), 2) as avg_requests_per_user,\n",
    "            MAX(total_requests) as max_requests_by_user\n",
    "        FROM lakehouse.gold.user_metrics\n",
    "    \"\"\").show()\n",
    "except Exception as e:\n",
    "    print(f\"   (user_metrics not available: {e})\")\n",
    "\n",
    "print(\"\\n‚úÖ Analytics complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

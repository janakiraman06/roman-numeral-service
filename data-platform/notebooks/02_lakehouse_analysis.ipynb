{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèîÔ∏è Lakehouse Analysis with PySpark + Iceberg\n",
        "\n",
        "This notebook demonstrates querying the Iceberg Lakehouse tables using PySpark.\n",
        "\n",
        "## Medallion Architecture Layers\n",
        "- **Bronze**: Raw events from Kafka (ingested by Flink)\n",
        "- **Silver**: Cleaned, deduplicated facts and SCD Type 2 dimensions\n",
        "- **Gold**: Aggregated metrics for BI/Analytics\n",
        "\n",
        "## Prerequisites\n",
        "Run the medallion flow test first:\n",
        "```bash\n",
        "docker exec spark-master /opt/spark/bin/spark-submit \\\n",
        "    --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \\\n",
        "    --conf spark.extraListeners= \\\n",
        "    /opt/spark-jobs/test_medallion_flow.py\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configure Spark for Iceberg on MinIO using Hadoop catalog\n",
        "# (No Hive Metastore required - uses file-based catalog)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Lakehouse Analysis\") \\\n",
        "    .config(\"spark.jars.packages\", \n",
        "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\"\n",
        "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
        "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.lakehouse\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.lakehouse.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.lakehouse.warehouse\", \"s3a://lakehouse/warehouse\") \\\n",
        "    .config(\"spark.sql.catalog.lakehouse.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(f\"‚úÖ Spark version: {spark.version}\")\n",
        "print(f\"üì¶ Iceberg catalog configured: lakehouse (Hadoop catalog)\")\n",
        "print(f\"ü™£ MinIO endpoint: http://minio:9000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List tables in each Medallion layer\n",
        "for layer in ['bronze', 'silver', 'gold']:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìÅ {layer.upper()} LAYER\")\n",
        "    print('='*60)\n",
        "    try:\n",
        "        tables = spark.sql(f\"SHOW TABLES IN lakehouse.{layer}\")\n",
        "        if tables.count() == 0:\n",
        "            print(\"   (no tables)\")\n",
        "        else:\n",
        "            for row in tables.collect():\n",
        "                table_name = f\"lakehouse.{layer}.{row['tableName']}\"\n",
        "                count = spark.table(table_name).count()\n",
        "                print(f\"   ‚îî‚îÄ‚îÄ {row['tableName']}: {count} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query Gold layer tables\n",
        "print(\"üìä GOLD LAYER ANALYTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Daily Summary\n",
        "try:\n",
        "    daily_df = spark.table(\"lakehouse.gold.daily_conversion_summary\")\n",
        "    print(\"\\nüìÖ Daily Conversion Summary:\")\n",
        "    daily_df.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå daily_conversion_summary: {e}\")\n",
        "\n",
        "# User Metrics\n",
        "try:\n",
        "    users_df = spark.table(\"lakehouse.gold.user_metrics\")\n",
        "    print(\"\\nüë• User Metrics:\")\n",
        "    users_df.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå user_metrics: {e}\")\n",
        "\n",
        "# Popular Numbers\n",
        "try:\n",
        "    popular_df = spark.table(\"lakehouse.gold.popular_numbers\")\n",
        "    print(\"\\nüî• Most Popular Numbers:\")\n",
        "    popular_df.show(truncate=False)\n",
        "    \n",
        "    # Visualization\n",
        "    pdf = popular_df.toPandas()\n",
        "    if len(pdf) > 0:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        ax.barh(pdf['output_value'].astype(str), pdf['request_count'], color='steelblue')\n",
        "        ax.set_xlabel('Request Count')\n",
        "        ax.set_ylabel('Roman Numeral')\n",
        "        ax.set_title('Most Popular Roman Numeral Conversions')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå popular_numbers: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
